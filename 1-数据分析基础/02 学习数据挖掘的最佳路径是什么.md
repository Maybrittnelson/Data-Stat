[TOC]



## 数据挖掘的基本流程

1. **商业理解：**理解业务
2. **数据理解：**尝试收集部分数据，然后对数据进行探索，包括数据描述、数据质量验证等。
3. **数据准备：**开始收集数据，并对数据进行清洗、、数据集成等操作。
4. **模型建立：**选择和应用各种数据挖掘模型，并进行优化，以便得到更好的分类结果。
5. **模型评估：**对模型进行评价，并检查构建模型的每个步骤，确认模型是否实现了预定的商业目标。
6. **上线发布：**模型的作用是从数据中找到金矿，也就是我们所说的“知识”，获得的知识需要转化成用户可以使用的方式，呈现的形式可以是一份报告，也可以是实现一个比较复杂的、可重复的数据挖掘过程。数据挖掘结果如果日常运营的一部分，那么后续的监控和维护就会变得重要。

## 数据挖掘的十大算法

按照不同的目的，我可以讲这些算法分成四类，以便你跟好的理解。

> **分类算法：**C4.5，朴素贝叶斯（Naive Bayes），SVM，KNN，Adaboost，CART
>
> **聚类算法：**K-Means，EM
>
> **关联分析：**Apriori
>
> **连接分析：**PageRank

### 1 C4.5

C4.5 算法是得票最高的算法，可以说是十大算法之首。C4.5是决策树的算法，它创造性地在决策树构造过程中就进行了简直，并且可以处理连续的属性，也能对不完整的数据进行处理。它可以说是决策树分类中，具有里程碑式意义的算法。

### 2 朴素贝叶斯（Naive Bayes）

朴素贝叶斯模型是基于概率论的原理，它的思想是这样的：对于给出的未知物体想要进行分类，就需要求解在这个未知物体出现的条件下各个类别出现的概率，哪个最大，就认为这个未知物体属于哪个分类。

### 3 SVM

SVM 的中文叫支持向量机，英文是Support Vector Machine，简称SVM。SVM在训练建立了一个超平面的分类模型。如果你对超平面不理解，没有关系，我在后面的算法篇会给你进行介绍。

### 4 KNN

KNN 也叫K最近邻算法，英文是 K-Nearest-Neighbor。所谓 K 近邻，就是每个样本都可以用它最接近的K个邻居来代表。如果一个样本，它的K个最接近的邻居都属于分类A，那么这个样本也属于分类A。

### 5 AdaBoost

Adaboost 在训练中建立了一个联合的分类模型。boost在英文中代表提升的意思，所以Adaboost是个构建分类器的提升算法。它可以让我们多个弱的分类器组成一个强的分类器，所以Adaboost 也是一个常用的分类算法。

### 6 CART

CART 代表分类和回归树，英文是 Classification and Regression Trees。像英文一样，它构建了两棵树：一颗是分类树，另一个是回归树。和C4.5一样，它是一个决策树学习方法。

### 7 Apriori

Apriori 是一种挖掘关联规则（association rules）的算法，它通过挖掘频繁项集（frequent item sets）来揭示物品之间的关联关系，被广泛应用到商业挖掘和网络安全等领域中。频繁项集是指经常出现在一起的物品的集合，关联规则暗示着两种物品之间可能存在很强的关系。

### 8 K-Means

K-Means 算法是一个聚类算法。你可以这么理解，最终我想把物体划分成K类。假设每个类别里面，都有个“中心点”，即意见领袖，它是这个类别的核心。现在我有一个新点要归类，这时候就只要计算这个新点与K个中新点的距离，距离哪个中心点近，就变成了哪个类别。

### 9 EM

EM 算法也叫最大期望算法，是求参数的最大似然估计的一种方法。原理是这样的：假设我们想评估参数A 和 参数B，在开始状态下二者都是未知的，并且知道了 A 的信息就可以得到B的信息，反过来就知道了 B 也就得到了 A。可以考虑首先赋予 A 某个初值，以此得到B的估值，然后从B 的估值出发，重新估计 A 的取值，这个过程一直持续到收敛为止。

EM 算法经常用于聚类和机器学习领域中。

### 10 PageRank

PageRank 起源于论文影响力的计算方式，如果一片文论被引入的次数越多，就代表着篇论文的影响力越强。同样PageRank 被 Google创造性地应用到了网页权重的计算中：当一个页面链出的页面越多，说明这个页面的“参考文献”越多，当这个页面被链入的频率越高，说明这个页面被引用的次数越高。基于这个原理，我们可以得到网站的权重划分。

## 数据挖掘的数学原理

我说了这么多数据挖掘中的经典算法，但是如果你不了解概率论和数理统计，还是很难掌握算法的本质：

如果你不懂线性代数，就很难理解矩阵和向量运作在数据挖掘中的价值；

如果你没有最优化的方法的概念，就对迭代收敛理解不深。

### 1. 概率论与数理统计

大学时的概率论，偏概率，统计部分少点。在数据挖掘里使用到概率论的地方就比较多了。比如条件概率、独立性的概念，以及随机变量、多维随机变量的概念。

### 2. 线性代数

向量和矩阵是线性代数中的重要知识点，它被广泛应用到数据挖掘中，比如我们经常会把对想抽象为矩阵的表示，一幅图像就可以抽象出来是一个矩阵，我们也经常计算特征值和特征向量，用特征向量来近似代表物体的特征。这个是大数据降维的基本思路。

基于矩阵的各种运算，以及基于矩阵的理论成熟，可以帮我们解决很多实际问题，比如 PCA 方法、SVD 方法，以及 MF、NMF 方法等在数据挖掘中都有广泛的应用。

### 3. 图论

社交网络的兴起，让图论的应用也越来越广。人与人的关系，可以用图论上的两个节点来进行连接，节点的度可以理解为一个人的朋友数。我们都听说过人脉的六度理论，在Facebook 上被证明平均一个人与另一个人的连接，只需要3.57个人。当然图论对于网络结构的分析非常有效，同时图论也在关系挖掘和图像分割中有重要的作用。

### 4. 最优化方法

最优化方法相当于机器学习中自我学习的过程，当机器知道了目标，训练后与结果存在偏差就需要迭代调整，那么最优化就是这个调整的过程。一般来说，这个学习和迭代的过程是很漫长、随机的。最优化方法的提出就是用更短的时间的得到收敛，取得更好的效果。

## 总结

![img](https://static001.geekbang.org/resource/image/11/d7/1130af6d29d029c470144dfc8610b6d7.jpg)

